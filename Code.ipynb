{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fa03e8",
   "metadata": {},
   "source": [
    "## Dataset links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e7c4e",
   "metadata": {},
   "source": [
    "Ryerson Audio-Visual Database of Emotional Speech and Song [RAVDESS](https://smartlaboratory.org/ravdess/)\n",
    "\n",
    "Crowd-sourced Emotional Mutimodal Actors Dataset [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/)\n",
    "\n",
    "Surrey Audio-Visual Expressed Emotion [TESS](https://tspace.library.utoronto.ca/handle/1807/24487)\n",
    "\n",
    "Toronto emotional speech set [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f5afa",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "from IPython.core.display import display\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras import layers, models, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# setting seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6a634",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd828fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS = \"./Datasets/TESS/\"\n",
    "RAV = \"./Datasets/RAVDESS/\"\n",
    "SAVEE = \"./Datasets/SAVEE/\"\n",
    "CREMA = \"./Datasets/CREMA-D/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b66a3",
   "metadata": {},
   "source": [
    "### Loading SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('male_angry')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('male_disgust')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('male_fear')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('male_happy')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('male_neutral')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('male_sad')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('male_surprise')\n",
    "    path.append(SAVEE + i)\n",
    "    \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7240700",
   "metadata": {},
   "source": [
    "### Loading RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(RAV)\n",
    "\n",
    "emotion=[]\n",
    "gender=[]\n",
    "path=[]\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(RAV + i)\n",
    "    for f in fname:\n",
    "        part = f.split(\".\")[0].split(\"-\")\n",
    "        emotion.append(int(part[2]))\n",
    "        if int(part[6])%2 == 0:\n",
    "            gender.append(\"female\")\n",
    "        else:\n",
    "            gender.append(\"male\")\n",
    "        path.append(RAV + i + \"/\" + f)\n",
    "        \n",
    "RAV_df = pd.DataFrame(emotion, columns=[\"emotions\"])\n",
    "RAV_df = RAV_df.replace({1:\"neutral\",2:\"calm\",3:\"happy\",4:\"sad\",5:\"angry\",6:\"fear\",7:\"disgust\",8:\"surprise\"})\n",
    "RAV_df = pd.concat([RAV_df, pd.DataFrame(gender, columns=[\"gender\"])], axis=1)\n",
    "RAV_df[\"labels\"] = RAV_df[\"gender\"] + \"_\" + RAV_df[\"emotions\"]\n",
    "RAV_df = pd.concat([RAV_df, pd.DataFrame(path, columns=[\"path\"])], axis=1)\n",
    "RAV_df = RAV_df.drop([\"emotions\", \"gender\"], axis=1)\n",
    "RAV_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad313d7d",
   "metadata": {},
   "source": [
    "### Loading TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        part = f.split(\".\")[0].split(\"_\")\n",
    "        if part[2]==\"angry\":\n",
    "            emotion.append(\"female_angry\")\n",
    "        elif part[2]==\"disgust\":\n",
    "            emotion.append(\"female_disgust\")\n",
    "        elif part[2]==\"fear\":\n",
    "            emotion.append(\"female_fear\")\n",
    "        elif part[2]==\"happy\":\n",
    "            emotion.append(\"female_happy\")\n",
    "        elif part[2]==\"neutral\":\n",
    "            emotion.append(\"female_neutral\")\n",
    "        elif part[2]==\"ps\":\n",
    "            emotion.append(\"female_surprise\")\n",
    "        elif part[2]==\"sad\":\n",
    "            emotion.append(\"female_sad\")\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "            \n",
    "TESS_df = pd.DataFrame(emotion, columns=[\"labels\"])\n",
    "TESS_df = pd.concat([TESS_df, pd.DataFrame(path, columns=[\"path\"])], axis=1)\n",
    "TESS_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9765f34",
   "metadata": {},
   "source": [
    "### Loading CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(CREMA)\n",
    "dir_list.sort()\n",
    "\n",
    "gender = []\n",
    "emotion = []\n",
    "path = []\n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "for i in dir_list: \n",
    "    part = i.split('_')\n",
    "    if int(part[0]) in female:\n",
    "        temp = 'female'\n",
    "    else:\n",
    "        temp = 'male'\n",
    "    gender.append(temp)\n",
    "    if part[2] == 'SAD' and temp == 'male':\n",
    "        emotion.append('male_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'male':\n",
    "        emotion.append('male_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'male':\n",
    "        emotion.append('male_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'male':\n",
    "        emotion.append('male_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'male':\n",
    "        emotion.append('male_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'male':\n",
    "        emotion.append('male_neutral')\n",
    "    elif part[2] == 'SAD' and temp == 'female':\n",
    "        emotion.append('female_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'female':\n",
    "        emotion.append('female_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'female':\n",
    "        emotion.append('female_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'female':\n",
    "        emotion.append('female_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'female':\n",
    "        emotion.append('female_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'female':\n",
    "        emotion.append('female_neutral')\n",
    "    else:\n",
    "        emotion.append('Unknown')\n",
    "    path.append(CREMA + i)\n",
    "    \n",
    "CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "CREMA_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91363799",
   "metadata": {},
   "source": [
    "## Concatenating all 4 datasets and resetting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(\"index\", inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af43951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae6ed5",
   "metadata": {},
   "source": [
    "## Analysing and visualising example audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f142ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_rate = librosa.load(\"C:/Users/yashr/Desktop/Audio Project/Datasets/SAVEE/DC_a01.wav\", res_type='kaiser_fast', sr=44100)\n",
    "ipd.Audio(data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47dc70",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = librosa.stft(data)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "librosa.display.specshow(spec, x_axis='time', y_axis='mel')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee1f6c",
   "metadata": {},
   "source": [
    "### Mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128,fmax=12000) \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "librosa.display.specshow(mel_spec, x_axis='time', y_axis='mel')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04a0b7",
   "metadata": {},
   "source": [
    "### Mel-spectrogram on decibel scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87623834",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_spec = librosa.power_to_db(mel_spec, ref=np.min)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "librosa.display.specshow(db_spec, x_axis='time', y_axis='mel')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0f6fb",
   "metadata": {},
   "source": [
    "## Datasets split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87032e",
   "metadata": {},
   "source": [
    "### Splitting data on target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42, stratify=df.labels)\n",
    "val, test = train_test_split(test, test_size=0.2, random_state=42, stratify=test.labels)\n",
    "\n",
    "\n",
    "labels_train = train.labels.values.tolist()\n",
    "labels_val = val.labels.values.tolist()\n",
    "labels_test = test.labels.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90b4a7",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df01a0e",
   "metadata": {},
   "source": [
    "#### Below three cells convert the complete data into mel spectrograms and then reads back as pixel values into arrays and saves those arrays into train and test lists.Takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10949751",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matplotlib.use('Agg') #Does not let the figures show in cells. Saves time while converting complete data\n",
    "\n",
    "image_path = \"./mel-spec-images/\"\n",
    "train_path = \"train/\"\n",
    "test_path = \"test/\"\n",
    "val_path = \"val/\"\n",
    "for i in train_path,test_path, val_path:\n",
    "    if not os.path.exists(image_path + i):\n",
    "        os.makedirs(image_path + i)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for index,path in enumerate(train.path):\n",
    "    '''converting audio data into spectrogram and saving on HD'''\n",
    "    data, sample_rate = librosa.load(path, res_type='kaiser_fast', sr=44100)\n",
    "    spectrogram = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128,fmax=12000) \n",
    "    db_spec = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    fig = plt.figure(figsize=(8,6), num=1, clear=True)\n",
    "    librosa.display.specshow(db_spec)\n",
    "    plt.savefig(image_path + train_path + str(index) + \".png\")  \n",
    "    \n",
    "    '''Loading saved spectrogram as array'''\n",
    "    image=tf.keras.preprocessing.image.load_img(image_path + train_path + str(index) + \".png\", color_mode='rgb', target_size= (128,128))\n",
    "    image=np.array(image)\n",
    "    X_train.append(image)\n",
    "    y_train.append(labels_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956dd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for index,path in enumerate(val.path):\n",
    "    '''converting audio data into spectrogram and saving on HD'''\n",
    "    data, sample_rate = librosa.load(path, res_type='kaiser_fast', sr=44100)\n",
    "    spectrogram = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128,fmax=12000) \n",
    "    db_spec = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    fig = plt.figure(figsize=(8,6), num=1, clear=True)\n",
    "    librosa.display.specshow(db_spec)\n",
    "    plt.savefig(image_path + val_path + str(index) + \".png\")  \n",
    "    \n",
    "    '''Loading saved spectrogram as array'''\n",
    "    image=tf.keras.preprocessing.image.load_img(image_path + val_path + str(index) + \".png\", color_mode='rgb', target_size= (128,128))\n",
    "    image=np.array(image)\n",
    "    X_val.append(image)\n",
    "    y_val.append(labels_val[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c83076",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for index,path in enumerate(test.path):\n",
    "    '''converting audio data into spectrogram and saving on HD'''\n",
    "    data, sample_rate = librosa.load(path, res_type='kaiser_fast', sr=44100)\n",
    "    spectrogram = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128,fmax=12000) \n",
    "    db_spec = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    fig = plt.figure(figsize=(8,6), num=1, clear=True)\n",
    "    librosa.display.specshow(db_spec)\n",
    "    plt.savefig(image_path + test_path + str(index) + \".png\") \n",
    "    \n",
    "    '''Loading saved spectrogram as array'''\n",
    "    image=tf.keras.preprocessing.image.load_img(image_path + test_path + str(index) + \".png\", color_mode='rgb', target_size= (128,128))\n",
    "    image=np.array(image)\n",
    "    X_test.append(image)\n",
    "    y_test.append(labels_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instances in X_train:\", len(X_train))\n",
    "print(\"Instances in X_val:\", len(X_val))\n",
    "print(\"Instances in X_test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list into arrays and normalise the pixel values between 0 and 1\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12691c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of X_train: \", X_train.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of X_val: \", X_val.shape)\n",
    "print(\"shape of y_val: \", y_val.shape)\n",
    "print(\"shape of X_test: \", X_test.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ed55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target variables\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = to_categorical(lb.fit_transform(y_train))\n",
    "y_val = to_categorical(lb.fit_transform(y_val))\n",
    "y_test = to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final labels as a separate variable for use when we are loading the array data directly from HD \n",
    "#since label encoder won't work in inverse transforming when we load the arrays from HD\n",
    "\n",
    "classes = np.array(['female_angry', 'female_calm', 'female_disgust', 'female_fear',\n",
    "                   'female_happy', 'female_neutral', 'female_sad', 'female_surprise',\n",
    "                   'male_angry', 'male_calm', 'male_disgust', 'male_fear',\n",
    "                   'male_happy', 'male_neutral', 'male_sad', 'male_surprise'],\n",
    "                  dtype='<U15')\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the arrays into HD for future use\n",
    "\n",
    "np.save(\"X_train\", X_train)\n",
    "np.save(\"X_val\", X_val)\n",
    "np.save(\"X_test\", X_test)\n",
    "np.save(\"y_train\", y_train)\n",
    "np.save(\"y_val\", y_val)\n",
    "np.save(\"y_test\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading numpy arrays from HD\n",
    "\n",
    "# X_train = np.load(\"./X_train.npy\")\n",
    "# X_val = np.load(\"./X_val.npy\")\n",
    "# X_test = np.load(\"./X_test.npy\")\n",
    "# y_train = np.load(\"./y_train.npy\")\n",
    "# y_val = np.load(\"./y_val.npy\")\n",
    "# y_test = np.load(\"./y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223c633",
   "metadata": {},
   "source": [
    "## Loading pre-trained VGG19 model and modifying the bottom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fedee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "vgg_model = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(128, 128, 3))\n",
    "\n",
    "model = Sequential()\n",
    "for layer in vgg_model.layers:\n",
    "    model.add(layer)\n",
    "model.add(Flatten())  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "learning_rate= 1e-6\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=5,\n",
    "                                                    verbose=0, mode=\"auto\", baseline=None,\n",
    "                                                    restore_best_weights=False)\n",
    "                                                \n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.4, patience=3, verbose=0)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_new_model\", monitor=\"val_accuracy\", verbose=0,\n",
    "                                                save_best_only=True, save_weights_only=False, \n",
    "                                                mode=\"auto\", save_freq=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 16, epochs=50, validation_data=(X_val,y_val), callbacks = [early_stopping, plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ca653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b96856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30939e",
   "metadata": {},
   "source": [
    "## Evaluating the model using confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=16, verbose=1)\n",
    "\n",
    "# Predicted labels\n",
    "\n",
    "preds = preds.argmax(axis=1)\n",
    "# preds = classes[preds]\n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predictedvalues': preds})\n",
    "\n",
    "# Actual labels\n",
    "\n",
    "actual = y_test.argmax(axis=1)\n",
    "# actual = classes[actual]\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actualvalues': actual})\n",
    "\n",
    "# Combining predicted and actual values in a single dataframe\n",
    "\n",
    "finaldf = actual.join(preds)\n",
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff830d44",
   "metadata": {},
   "source": [
    "### Overall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f609d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\") \n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(new_y_test, predictions)\n",
    "print_confusion_matrix(matrix, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_y_test, predictions, target_names=classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d6b7",
   "metadata": {},
   "source": [
    "### Gender identification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72947b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df = finaldf.copy()\n",
    "gender_df['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'female_calm':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                       , 'male_calm':'male'\n",
    "                                      })\n",
    "\n",
    "gender_df['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'female_calm':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                       , 'male_calm':'male'\n",
    "                                      })\n",
    "\n",
    "gender_classes = gender_df.actualvalues.unique()  \n",
    "gender_classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "matrix = confusion_matrix(gender_df.actualvalues, gender_df.predictedvalues)\n",
    "print(accuracy_score(gender_df.actualvalues, gender_df.predictedvalues))\n",
    "print_confusion_matrix(matrix, class_names = gender_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gender_df.actualvalues, gender_df.predictedvalues, target_names=gender_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a821f",
   "metadata": {},
   "source": [
    "### Emotion identification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faca5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_df = finaldf.copy()\n",
    "emotions_df['actualvalues'] = emotions_df.actualvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'female_calm':'calm'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                       , 'male_calm':'calm'\n",
    "                                      })\n",
    "\n",
    "emotions_df['predictedvalues'] = emotions_df.predictedvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'female_calm':'calm'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                       , 'male_calm':'calm'\n",
    "                                      })\n",
    "\n",
    "emotion_classes = emotions_df.actualvalues.unique() \n",
    "emotion_classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(emotions_df.actualvalues, emotions_df.predictedvalues)\n",
    "print(accuracy_score(emotions_df.actualvalues, emotions_df.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = emotion_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(emotions_df.actualvalues, emotions_df.predictedvalues, target_names=emotion_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245a0fa",
   "metadata": {},
   "source": [
    "## Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model.h5\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"best_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_file = open('./best_model.json', 'r')\n",
    "# model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# model = model_from_json(model_json)\n",
    "\n",
    "# model.load_weights(\"./best_model.h5\")\n",
    "\n",
    "# optimizer=optimizers.RMSprop(learning_rate=0.000001)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727687b",
   "metadata": {},
   "source": [
    "## Evaluating model on own voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43088fa",
   "metadata": {},
   "source": [
    "### Need to install and import below modules to record the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipwin\n",
    "!pipwin install pyaudio\n",
    "!pip install wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d1239",
   "metadata": {},
   "source": [
    "### The below function records the voice for 4 seconds and passes the recording though the trained model to be evaluated and outputs it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36796ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    '''Creating new wav file for predicting user audio'''\n",
    "    CHUNK = 1024 \n",
    "    FORMAT = pyaudio.paInt16 \n",
    "    CHANNELS = 2 \n",
    "    RATE = 44100 \n",
    "    RECORD_SECONDS = 4\n",
    "    WAVE_OUTPUT_FILENAME = \"./testing.wav\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"* recording\")\n",
    "    frames = []\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"* done recording\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    '''Converts the above recorded wav file into mel-spec and reads back into our Neural network readable format'''\n",
    "    data, sample_rate = librosa.load(\"./testing.wav\", res_type='kaiser_fast',sr=44100)\n",
    "    spectrogram = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128,fmax=12000) \n",
    "    db_spec = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    fig = plt.figure(figsize=(8,6), num=1, clear=True)\n",
    "    librosa.display.specshow(db_spec)\n",
    "    plt.savefig(\"./testing.png\")  \n",
    "    image=tf.keras.preprocessing.image.load_img(\"./testing.png\", color_mode='rgb', target_size= (128,128))\n",
    "    testing_file=np.array(image)\n",
    "    testing_file = testing_file.astype(\"float32\")\n",
    "    testing_file /= 255\n",
    "    testing_file = np.expand_dims(testing_file, axis=0)\n",
    "\n",
    "    '''Passing the converted file to the model for prediction'''\n",
    "    preds = model.predict(testing_file)\n",
    "    preds = preds.argmax(axis=1)\n",
    "#     preds = classes[preds]\n",
    "    preds = preds.astype(int).flatten()\n",
    "    preds = (lb.inverse_transform((preds)))\n",
    "    print(\"Voice predicted as: \", preds) \n",
    "\n",
    "    '''Listening to recorded audio'''\n",
    "    display(ipd.Audio('./testing.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff5a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
